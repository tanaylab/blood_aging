{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('dummy_to_replace_with_blood_aging_dir_path/code')\n",
    "# print(os.getcwd())\n",
    "\n",
    "\n",
    "import sys\n",
    "import shutil\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "\n",
    "import importlib\n",
    "import itertools\n",
    "import metacells as mc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.spatial.distance\n",
    "import matplotlib\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import logging\n",
    "import re\n",
    "import collections\n",
    "import sklearn\n",
    "import sklearn.compose\n",
    "import pickle\n",
    "\n",
    "# from generic import gene_module_utils\n",
    "from generic import generic_utils\n",
    "from generic import ml_utils\n",
    "from mds import mds_analysis_params\n",
    "from mds import mds_analysis\n",
    "# from mds import ipssm_utils\n",
    "from generic import mc_utils\n",
    "from mds import arch_mutation_interface_and_utils\n",
    "from mds import clinical_data_interface_and_utils\n",
    "from sc_rna_seq_preprocessing import sc_rna_seq_preprocessing_params\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = False\n",
    "plt.rcParams['patch.linewidth'] = 0\n",
    "plt.rcParams['patch.edgecolor'] = 'none'\n",
    "# plt.rcParams['scatter.edgecolors'] = 'black' # didnt affect sb.scatterplot\n",
    "\n",
    "%matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def get_mds_params():\n",
    "    return mds_analysis_params.MDS_ANALYSIS_PARAMS\n",
    "\n",
    "\n",
    "def get_sc_rna_seq_preprocessing_params():\n",
    "    return sc_rna_seq_preprocessing_params.SC_RNA_SEQ_PREPROCESSING_PARAMS\n",
    "\n",
    "plt.subplots()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_misc_numbered_donor_info = mds_analysis.get_all_misc_numbered_donor_info()\n",
    "\n",
    "xgboost_numbered_donor_df = pd.read_csv(get_mds_params()['all_numbered_donor_df_csv_file_path'])\n",
    "xgboost_ext_donor_feature_df = pd.read_csv(get_mds_params()['all_ext_donor_feature_df_csv_file_path'])\n",
    "xgboost_ext_donor_feature_df = xgboost_ext_donor_feature_df[xgboost_ext_donor_feature_df[['in_test_set', 'in_train_set']].any(axis=1)]\n",
    "\n",
    "diagnosis_classes_of_interest = [\n",
    "    'cytopenia',\n",
    "    'MDS',\n",
    "    'MDS/MPN',\n",
    "    'normal',\n",
    "]\n",
    "\n",
    "xgboost_numbered_donor_df = generic_utils.merge_preserving_df1_index_and_row_order(xgboost_numbered_donor_df, xgboost_ext_donor_feature_df[['numbered_donor_id', 'in_test_set', 'in_train_set']], how='left')\n",
    "xgboost_numbered_donor_df = xgboost_numbered_donor_df[xgboost_numbered_donor_df[['in_test_set', 'in_train_set']].any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cbc_10x_sample_days_dist = np.inf\n",
    "\n",
    "\n",
    "possible_composition_cols = {f'log_c_{x}' for x in mds_analysis_params.PB_HSPC_STATE_NAMES}\n",
    "c_composition_feature_cols = sorted(possible_composition_cols & set(xgboost_ext_donor_feature_df.columns))\n",
    "\n",
    "gene_prog_sig_names = list(get_mds_params()['pb_sig_name_to_info'])\n",
    "possible_gene_prog_cols = {f'{x}_{y}_0.5' for x, y in itertools.product(gene_prog_sig_names, mds_analysis_params.PB_HSPC_STATE_NAMES)}\n",
    "median_gene_prog_cols = sorted(possible_gene_prog_cols & set(xgboost_ext_donor_feature_df.columns))\n",
    "median_gene_prog_cols_without_hla = [x for x in median_gene_prog_cols if '_hla_sig_' not in x]\n",
    "hla_gene_prog_cols = sorted(set(median_gene_prog_cols) - set(median_gene_prog_cols_without_hla))\n",
    "print(f'hla_gene_prog_cols: {hla_gene_prog_cols}')\n",
    "binary_snv_cols = [x for x in xgboost_ext_donor_feature_df.columns if x.endswith('_snv')]\n",
    "\n",
    "possible_c_state_hsc_mpp_median_umi_count_log_ratio_cols = {f'c_{x}_HSC_MPP_median_umi_count_log_ratio' for x in mds_analysis_params.PB_HSPC_STATE_NAMES}\n",
    "c_state_hsc_mpp_median_umi_count_log_ratio_cols = sorted(possible_c_state_hsc_mpp_median_umi_count_log_ratio_cols & set(xgboost_ext_donor_feature_df.columns))\n",
    "\n",
    "cbc_cols = [\n",
    "    'Basophils#', 'Basophils%', 'Eosinophils#', 'Eosinophils%', 'Hematocrit (%)', 'Hemoglobin (g/dl)', 'Lympho#', 'Lympho%', 'MCH (pg)', 'MCHC (g/dl)', 'MCV (fL)', 'MPV (fL)', 'Mono#', 'Mono%', 'Neutro#', 'Neutro%', 'Platelets (10^3/microliter)', 'RBC (10^6/microliter)', 'RDW (%)', 'WBC (10^3/microliter)',\n",
    "]\n",
    "\n",
    "composition_gene_progs_and_cna_count_cols = [\n",
    "    *c_composition_feature_cols,\n",
    "    *median_gene_prog_cols_without_hla,\n",
    "    'cna_count',\n",
    "]\n",
    "final_features_without_age_and_sex_and_vaf_and_hla = [\n",
    "    *cbc_cols,\n",
    "    *c_composition_feature_cols,\n",
    "    *median_gene_prog_cols_without_hla,\n",
    "    'cna_count',\n",
    "    'mean_near_neighbor_dist',\n",
    "    \n",
    "    # *c_state_hsc_mpp_median_umi_count_log_ratio_cols,\n",
    "    # *mds_analysis_params.NEIGHBOR_STATE_LOG_RATIO_COLS,\n",
    "    # 'log_c_clp_e',\n",
    "]\n",
    "final_features_without_age_and_sex_and_vaf = [\n",
    "    *final_features_without_age_and_sex_and_vaf_and_hla,\n",
    "    *hla_gene_prog_cols,\n",
    "]\n",
    "final_features_without_age_and_sex = [\n",
    "    *final_features_without_age_and_sex_and_vaf,\n",
    "    'max_mean_VAF',\n",
    "]\n",
    "final_features_with_age_and_sex_and_vaf = [\n",
    "    *final_features_without_age_and_sex,\n",
    "    'donor_age',\n",
    "    'donor_sex',\n",
    "]\n",
    "\n",
    "feature_df_for_classifier = xgboost_ext_donor_feature_df[['numbered_donor_id', *final_features_with_age_and_sex_and_vaf]].copy()\n",
    "cbc_too_far_from_10x_mask = xgboost_ext_donor_feature_df['cbc_10x_sample_days_dist'] > max_cbc_10x_sample_days_dist\n",
    "feature_df_for_classifier.loc[cbc_too_far_from_10x_mask, cbc_cols] = np.nan\n",
    "\n",
    "feature_df_for_classifier['donor_sex'] = feature_df_for_classifier['donor_sex'].astype(str)\n",
    "assert feature_df_for_classifier['donor_sex'].isin(['male', 'female', 'nan']).all()\n",
    "\n",
    "feature_df_for_classifier['is_female'] = (feature_df_for_classifier['donor_sex'] == 'female').astype(int)\n",
    "feature_df_for_classifier['is_male'] = (feature_df_for_classifier['donor_sex'] == 'male').astype(int)\n",
    "feature_df_for_classifier.drop(columns='donor_sex', inplace=True)\n",
    "\n",
    "feature_df_for_classifier.set_index('numbered_donor_id', inplace=True)\n",
    "feature_df_for_classifier.index.name = None\n",
    "print(f'final feature_df_for_classifier.shape: {feature_df_for_classifier.shape}')\n",
    "feature_df_for_classifier = feature_df_for_classifier.T\n",
    "\n",
    "\n",
    "final_feature_names_and_types_df = pd.DataFrame([\n",
    "    *[{'name': x, 'type': 'CBC'} for x in cbc_cols],\n",
    "    *[{'name': x, 'type': 'gene_prog_excluding_mhc_ii'} for x in median_gene_prog_cols_without_hla],\n",
    "    *[{'name': x, 'type': 'mhc_ii_gene_prog'} for x in hla_gene_prog_cols],\n",
    "    *[{'name': x, 'type': 'log_c_state_freq'} for x in c_composition_feature_cols],\n",
    "    {'name': 'max_mean_VAF', 'type': 'DNA'},\n",
    "    {'name': 'donor_age', 'type': 'demographic'},\n",
    "    {'name': 'donor_sex', 'type': 'demographic'},\n",
    "    {'name': 'mean_near_neighbor_dist', 'type': 'mean_near_neighbor_dist'},\n",
    "    {'name': 'log_c_clp_e', 'type': 'log_c_clp_e'},\n",
    "    {'name': 'cna_count', 'type': 'cna_count'},\n",
    "])\n",
    "final_feature_names_and_types_df.to_csv(get_mds_params()['final_feature_names_and_types_df_csv_file_path'], index=False)\n",
    "\n",
    "with_vaf_classifier_final_feature_names_and_types_df = final_feature_names_and_types_df[\n",
    "    final_feature_names_and_types_df['name'].isin(final_features_without_age_and_sex)].reset_index(drop=True)\n",
    "assert len(with_vaf_classifier_final_feature_names_and_types_df) == len(final_features_without_age_and_sex)\n",
    "with_vaf_classifier_final_feature_names_and_types_df.to_csv(get_mds_params()['with_vaf_classifier_final_feature_names_and_types_df_csv_file_path'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_with_age_and_sex_and_vaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_vaf_feature_type_count_df = final_feature_names_and_types_df[final_feature_names_and_types_df['name'].isin(final_features_without_age_and_sex)]['type'].value_counts()\n",
    "print(f'with_vaf_feature_type_count_df:\\n{with_vaf_feature_type_count_df}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_na_df = generic_utils.merge_preserving_df1_index_and_row_order(feature_df_for_classifier.loc[final_features_without_age_and_sex].isna().any(axis=1).reset_index(name='any_na').rename(columns={'index': 'name'}), final_feature_names_and_types_df)\n",
    "feature_types_with_any_na = sorted(any_na_df[any_na_df['any_na']]['type'].unique())\n",
    "print(f'feature_types_with_any_na: {feature_types_with_any_na}')\n",
    "# any_na_df[['type', 'any_na']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir_path = 'temp/_xgboost_out'\n",
    "pathlib.Path(out_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_gene_indices = []\n",
    "\n",
    "skip_cv = True\n",
    "skip_cv = False\n",
    "\n",
    "exclude_final_select = True\n",
    "exclude_final_select = False\n",
    "\n",
    "hardcoded_gene_pool_feature_pval_df_csv_file_path = None\n",
    "\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('mds_vs_cyto', 0.05, 0.05)\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('all', 0.05, 0.1)\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('all', 0.05, 0.05)\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('mds_vs_cyto', 0.05, 0.2)\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('all', 0.05, 0.2)\n",
    "\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('all', 0.05, np.inf)\n",
    "select_features_by, gene_to_pool_fdr_alpha, final_feature_selection_fdr_alpha = ('mds_vs_cyto', 0.05, 0.1)\n",
    "\n",
    "    \n",
    "no_pooled_genes = False\n",
    "no_pooled_genes = True\n",
    "\n",
    "no_pooled_genes_repr = '_no_DEGs' if no_pooled_genes else ''\n",
    "\n",
    "if hardcoded_gene_pool_feature_pval_df_csv_file_path:\n",
    "    pool_feature_pval_df = pd.read_csv(hardcoded_gene_pool_feature_pval_df_csv_file_path)\n",
    "    if 0:\n",
    "        plt.close('all')\n",
    "        sb.histplot(pool_feature_pval_df['cv_count'], bins=20)\n",
    "        raise\n",
    "    pool_feature_pval_df = pool_feature_pval_df[pool_feature_pval_df['cv_count'] > 50]\n",
    "    hardcoded_up_genes = list(pool_feature_pval_df.loc[pool_feature_pval_df['diff_type'] == 'up', 'feature'])\n",
    "    hardcoded_down_genes = list(pool_feature_pval_df.loc[pool_feature_pval_df['diff_type'] == 'down', 'feature'])\n",
    "    assert not (set(hardcoded_up_genes) & set(hardcoded_down_genes))\n",
    "\n",
    "print('better not use age and sex because our cohort is biased for them')\n",
    "plt.close('all')\n",
    "illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float = {\n",
    "    k: (1.0 if (v in (\n",
    "        'MDS', \n",
    "        'MDS/MPN',\n",
    "    )) else 0)\n",
    "    for k, v in all_misc_numbered_donor_info['numbered_donor_id_to_diagnosis_class'].items()\n",
    "}\n",
    "\n",
    "gene_state_reprs = [\n",
    "    'CLP',\n",
    "    'MEBEMP_L',\n",
    "    'HSC_MPP',\n",
    "    'ERYP',\n",
    "    'BEMP', \n",
    "]\n",
    "\n",
    "common_prediction_plot_kwargs = dict(\n",
    "    name_to_hue=all_misc_numbered_donor_info['numbered_donor_id_to_diagnosis_class'], \n",
    "    palette=clinical_data_interface_and_utils.DIAGNOSIS_CLASS_TO_COLOR,\n",
    ")\n",
    "\n",
    "mds_mpn_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['diagnosis_class'] == 'MDS/MPN', 'numbered_donor_id'])\n",
    "mds_mds_mpn_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['diagnosis_class'].isin(['MDS', 'MDS/MPN']), 'numbered_donor_id'])\n",
    "mds_mds_mpn_cytopenia_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['diagnosis_class'].isin(['MDS', 'MDS/MPN', 'cytopenia']), 'numbered_donor_id'])\n",
    "non_mds_mds_mpn_nd_ids = list(xgboost_numbered_donor_df.loc[~xgboost_numbered_donor_df['diagnosis_class'].isin(['MDS', 'MDS/MPN']), 'numbered_donor_id'])\n",
    "\n",
    "assert select_features_by in {'all', 'mds_vs_cyto'}\n",
    "nd_ids_to_use_in_feature_selection = None if select_features_by == 'all' else mds_mds_mpn_cytopenia_nd_ids\n",
    "\n",
    "# raise RuntimeError('why isnt N184 in test? because she has a tech rep! is in train.')\n",
    "get_nd_id_to_col_val = lambda col: generic_utils.get_dict_mapping_one_df_column_to_other(feature_df_for_classifier, 'numbered_donor_id', col)\n",
    "\n",
    "ult_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['is_ultima'], 'numbered_donor_id'])\n",
    "\n",
    "train_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['in_train_set'], 'numbered_donor_id'])\n",
    "test_nd_ids = list(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['in_test_set'], 'numbered_donor_id'])\n",
    "\n",
    "model_name_to_info = {\n",
    "    f'predict_mds_with_vaf_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        # curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        curr_test_df=feature_df_for_classifier[test_nd_ids],\n",
    "        feature_col_subset=final_features_without_age_and_sex,\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(2, 5), False),\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(2, 15, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(8, 10, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "    f'predict_mds_without_vaf_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        # curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        curr_test_df=feature_df_for_classifier[test_nd_ids],\n",
    "        feature_col_subset=final_features_without_age_and_sex_and_vaf,\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(3, 6), False), # almost no difference\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(5, 15, 2), False), # 13 is better than 5\n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(0, 11, 2), False), # 0 is best\n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.arange(0, 11, 2), False), # varies, but 0 and 8 are good...\n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "\n",
    "\n",
    "    f'predict_mds_with_comp_and_gene_progs_and_cna_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        feature_col_subset=composition_gene_progs_and_cna_count_cols,\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(2, 5), False),\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(2, 15, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(8, 10, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "    f'predict_mds_with_vaf_without_hla_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        feature_col_subset=[*final_features_without_age_and_sex_and_vaf_and_hla, 'max_mean_VAF'],\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(2, 5), False),\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(2, 15, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(8, 10, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "    f'predict_mds_by_cbc_and_vaf_only_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        feature_col_subset=[*cbc_cols, 'max_mean_VAF'],\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(2, 5), False),\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(2, 15, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(8, 10, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "    f'predict_mds_by_cbc_only_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        feature_col_subset=cbc_cols,\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(2, 5), False),\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(2, 15, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(8, 10, 1), False), \n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "    f'predict_mds_without_vaf_and_hla_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}': dict(\n",
    "        curr_train_df=feature_df_for_classifier[train_nd_ids],\n",
    "        curr_test_df=feature_df_for_classifier[sorted(set(feature_df_for_classifier) & set(ult_nd_ids))],\n",
    "        feature_col_subset=final_features_without_age_and_sex_and_vaf_and_hla,\n",
    "        # excluded_nd_ids=complex_karyo_nd_ids if exclude_complex_karyo else [], \n",
    "        nd_id_to_y=illu_ult_numbered_donor_id_to_is_mds_or_mds_mpn_as_float,\n",
    "        nd_ids_to_use_in_feature_selection=nd_ids_to_use_in_feature_selection,\n",
    "            \n",
    "        # arg_name_and_vals_and_plot_log=('max_depth', range(3, 6), False), # almost no difference\n",
    "        # arg_name_and_vals_and_plot_log=('n_estimators', range(5, 15, 2), False), # 13 is better than 5\n",
    "        # arg_name_and_vals_and_plot_log=('gamma', np.arange(0, 11, 2), False), # 0 is best\n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.arange(0, 11, 2), False), # varies, but 0 and 8 are good...\n",
    "        # arg_name_and_vals_and_plot_log=('min_child_weight', np.logspace(0.3, 1, 20), True),\n",
    "        # cv_kwargs=dict(\n",
    "        #     n_estimators=10, # sklearn default is 100. native api default is 10, it seems?. see https://stackoverflow.com/questions/68803420/what-is-the-default-value-of-n-estimators-in-xgboost-model/68806086#68806086, https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py, https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.train (num_boost_round)\n",
    "        #     max_depth=2, # IIUC, the default is 6 (https://xgboost.readthedocs.io/en/stable/parameter.html)\n",
    "        # ),\n",
    "    ),\n",
    "}\n",
    "\n",
    "model_name_to_cv_res = {}\n",
    "for model_name in [\n",
    "    f'predict_mds_with_vaf_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "    f'predict_mds_without_vaf_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "\n",
    "    # f'predict_mds_with_comp_and_gene_progs_and_cna_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "    # f'predict_mds_with_vaf_without_hla_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "    # f'predict_mds_by_cbc_and_vaf_only_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "    # f'predict_mds_by_cbc_only_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "    # f'predict_mds_without_vaf_and_hla_select_by_{select_features_by}_final_fdr_{final_feature_selection_fdr_alpha}{no_pooled_genes_repr}',\n",
    "]:\n",
    "    model_info = model_name_to_info[model_name]\n",
    "    curr_train_df = model_info['curr_train_df']\n",
    "    curr_test_df = model_info['curr_test_df']\n",
    "    nd_id_to_y = model_info['nd_id_to_y']\n",
    "\n",
    "    is_classif = model_info.get('is_classif', True)\n",
    "    random_state = model_info.get('random_state', 0)\n",
    "    feature_col_subset = model_info.get('feature_col_subset', None)\n",
    "    nd_ids_to_use_in_feature_selection = model_info.get('nd_ids_to_use_in_feature_selection', None)\n",
    "    excluded_nd_ids = model_info.get('excluded_nd_ids', [])\n",
    "    arg_name_and_vals_and_plot_log = model_info.get('arg_name_and_vals_and_plot_log', ('dummy', [0], False))\n",
    "    cv_kwargs = model_info.get('cv_kwargs', {})\n",
    "        \n",
    "    if curr_test_df is not None:\n",
    "        train_donor_ids = set(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['numbered_donor_id'].isin(curr_train_df.columns), 'donor_id'])\n",
    "        test_donor_ids = set(xgboost_numbered_donor_df.loc[xgboost_numbered_donor_df['numbered_donor_id'].isin(curr_test_df.columns), 'donor_id'])\n",
    "        common_donor_ids = train_donor_ids & test_donor_ids\n",
    "    \n",
    "        assert (curr_test_df.index == curr_train_df.index).all()\n",
    "    assert not common_donor_ids, f'train and test common_donor_ids: {common_donor_ids}'\n",
    "\n",
    "    \n",
    "    \n",
    "    curr_train_df = curr_train_df[curr_train_df.columns[~curr_train_df.columns.isin(excluded_nd_ids)]]\n",
    "    if curr_test_df is not None:\n",
    "        curr_test_df = curr_test_df[curr_test_df.columns[~curr_test_df.columns.isin(excluded_nd_ids)]]\n",
    "    if feature_col_subset is not None:\n",
    "        curr_train_df = curr_train_df.loc[feature_col_subset]\n",
    "        if curr_test_df is not None:\n",
    "            curr_test_df = curr_test_df.loc[feature_col_subset]\n",
    "\n",
    "    print(model_name, f'len(curr_train_df.columns): {len(curr_train_df.columns)}')\n",
    "    if curr_test_df is not None:\n",
    "        print(f'len(curr_test_df.columns): {len(curr_test_df.columns)}')\n",
    "    # print(f'feature_cols: {curr_train_df.index}')\n",
    "\n",
    "    nd_ids_to_use_in_feature_selection_mask = np.array(curr_train_df.columns.isin(nd_ids_to_use_in_feature_selection)) if (\n",
    "        nd_ids_to_use_in_feature_selection is not None) else None\n",
    "\n",
    "    train_target_vec = np.array([nd_id_to_y[x] for x in curr_train_df.columns])\n",
    "    # train_target_vec = np.random.permutation(np.array([nd_id_to_y[x] for x in curr_train_df.columns]))\n",
    "    train_target_nan_mask = np.isnan(train_target_vec)\n",
    "    nan_train_target_count = train_target_nan_mask.sum()\n",
    "    if nan_train_target_count > 0:\n",
    "        print(f'discarding ({nan_train_target_count}) train samples with nan target ({curr_train_df.columns[train_target_nan_mask]})')\n",
    "        curr_train_df = curr_train_df.loc[:, ~train_target_nan_mask]\n",
    "        train_target_vec = train_target_vec[~train_target_nan_mask]\n",
    "\n",
    "    if curr_test_df is None:\n",
    "        test_target_vec = None\n",
    "    else:\n",
    "        test_target_vec = np.array([nd_id_to_y[x] for x in curr_test_df.columns])\n",
    "        test_target_nan_mask = np.isnan(test_target_vec)\n",
    "        nan_test_target_count = test_target_nan_mask.sum()\n",
    "        if nan_test_target_count > 0:\n",
    "            print(f'discarding ({nan_test_target_count}) test samples with nan target ({curr_test_df.columns[test_target_nan_mask]})')\n",
    "            curr_test_df = curr_test_df.loc[:, ~test_target_nan_mask]\n",
    "            test_target_vec = test_target_vec[~test_target_nan_mask]\n",
    "\n",
    "    score_train_subgroup_name_to_mask = {}\n",
    "    score_train_subgroup_name_to_mask = {'mds_cytopenia_only': np.array(curr_train_df.columns.isin(mds_mds_mpn_cytopenia_nd_ids))}\n",
    "    score_train_subgroup_name_for_best_cv = 'all'\n",
    "    score_train_subgroup_name_for_best_cv = 'mds_cytopenia_only'\n",
    "    \n",
    "    if hardcoded_gene_pool_feature_pval_df_csv_file_path:\n",
    "        def curr_get_col_scores_and_masks_func(x, y, **kwargs):\n",
    "            return np.full(x.shape[1], np.nan), [x.columns.isin(hardcoded_up_genes), x.columns.isin(hardcoded_down_genes)]\n",
    "    else:\n",
    "        def curr_get_col_scores_and_masks_func(x, y, **kwargs):\n",
    "            return ml_utils.get_y_pos_and_neg_associated_feature_masks(x, y, fdr_alpha=gene_to_pool_fdr_alpha, ignore_nans=True, **kwargs)\n",
    "    \n",
    "    def curr_modify_pipe_inside_cv(pipe, train_mask):\n",
    "        if nd_ids_to_use_in_feature_selection_mask is not None:\n",
    "            train_sample_mask = nd_ids_to_use_in_feature_selection_mask[train_mask]\n",
    "            \n",
    "            pipe['select'].set_params(get_scores_and_support_mask_func_kwargs=dict(sample_mask=train_sample_mask))\n",
    "            for t in pipe['select_and_pool_diff_exp_genes'].transformers:\n",
    "                if not isinstance(t[1], ml_utils.PassthroughTransformer):\n",
    "                    t[1].set_params(get_col_scores_and_masks_func_kwargs=dict(sample_mask=train_sample_mask))\n",
    "\n",
    "    def xgboost_get_pipe(clf, exclude_final_select=False):\n",
    "        col_transformer = sklearn.compose.ColumnTransformer(\n",
    "            [(f'{st}_pooled_diff_genes', ml_utils.poolColsTransformer(['up', 'down'], get_col_scores_and_masks_func=curr_get_col_scores_and_masks_func), curr_train_df.index.isin([g for g in all_gene_indices if g.endswith(f'_{st}')])) for st in gene_state_reprs] + [\n",
    "                (x, ml_utils.PassthroughTransformer(['']), curr_train_df.index == x) for x in curr_train_df.index[~curr_train_df.index.isin(all_gene_indices)]\n",
    "            ],\n",
    "        )\n",
    "        col_transformer.set_output(transform='pandas')\n",
    "        final_select_as_list = [] if exclude_final_select else [\n",
    "            ('select', ml_utils.SelectorAllowingNans(lambda x, y, **kwargs: ml_utils.get_y_associated_feature_mask(x, y, fdr_alpha=final_feature_selection_fdr_alpha, ignore_nans=True, also_return_corrected_pvals=True, **kwargs)))\n",
    "        ]\n",
    "        pipe = sklearn.pipeline.Pipeline(steps=[\n",
    "            ('select_and_pool_diff_exp_genes', col_transformer),\n",
    "            *final_select_as_list,\n",
    "            ('clf', clf),\n",
    "        ])\n",
    "        return pipe\n",
    "\n",
    "    xgboost_cv_kwargs = dict(\n",
    "        is_classif=is_classif,\n",
    "        train_feature_df=curr_train_df.T,\n",
    "        train_target_vec=train_target_vec,\n",
    "        \n",
    "        tree_method=\"hist\", \n",
    "        # early_stopping_rounds=5,\n",
    "        # min_child_weight=1, # default\n",
    "        # eval_metric='logloss',\n",
    "        arg_name_and_vals_and_plot_log=arg_name_and_vals_and_plot_log,\n",
    "        score_train_subgroup_name_to_mask=score_train_subgroup_name_to_mask,\n",
    "        score_train_subgroup_name_for_best_cv=score_train_subgroup_name_for_best_cv,\n",
    "        \n",
    "        random_state=(random_state + 123),\n",
    "        seed=0,\n",
    "        nthread=1,\n",
    "        cv=sklearn.model_selection.LeaveOneOut(),\n",
    "        title_prefix_for_plots=f'{model_name}, ',\n",
    "        \n",
    "        test_feature_df=None if curr_test_df is None else curr_test_df.loc[curr_train_df.index].T,\n",
    "        test_target_vec=None if curr_test_df is None else np.array([nd_id_to_y[x] for x in curr_test_df.columns]),\n",
    "        prediction_barplot_kwargs=dict(\n",
    "            **common_prediction_plot_kwargs,\n",
    "            show_prediction_plus1=True,\n",
    "        ),\n",
    "        prediction_scatterplot_kwargs=dict(\n",
    "            **common_prediction_plot_kwargs,\n",
    "            show_names=True,\n",
    "        ),\n",
    "        get_pipe=lambda clf: xgboost_get_pipe(clf, exclude_final_select=exclude_final_select),\n",
    "        skip_cv=skip_cv,\n",
    "        out_dir_path=out_dir_path,\n",
    "        **cv_kwargs,\n",
    "    )\n",
    "    if hardcoded_gene_pool_feature_pval_df_csv_file_path and (not no_pooled_genes):\n",
    "        xgboost_cv_kwargs['title_prefix_for_plots'] += 'hardcoded pooled genes, '\n",
    "    if is_classif:\n",
    "        xgboost_cv_kwargs = {\n",
    "            **xgboost_cv_kwargs,\n",
    "            **dict(\n",
    "                objective='binary:logistic',\n",
    "                modify_pipe_inside_cv_func=curr_modify_pipe_inside_cv,\n",
    "            )\n",
    "        }\n",
    "    cv_res = ml_utils.xgboost_cv(**xgboost_cv_kwargs)\n",
    "    # print('auc:', cv_res)\n",
    "\n",
    "    model_name_to_cv_res[model_name] = cv_res\n",
    "    cv_res['model_name'] = model_name\n",
    "    \n",
    "all_cv_res_df = pd.concat([y['cv_res_df'].assign(model_name=x) for x, y in model_name_to_cv_res.items()], ignore_index=True)\n",
    "all_cv_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mds_and_cytopenia_df(df):\n",
    "    return df[df['name'].isin(mds_mds_mpn_cytopenia_nd_ids)]\n",
    "\n",
    "for model_name, cv_res in model_name_to_cv_res.items():\n",
    "    plt.close('all')\n",
    "    fig, ax = ml_utils.plot_auc({\n",
    "        'train': get_mds_and_cytopenia_df(cv_res['train_prediction_df']), \n",
    "        # 'train': cv_res['train_prediction_df'], \n",
    "        'test': get_mds_and_cytopenia_df(cv_res['test_prediction_df']),\n",
    "        # 'test': cv_res['test_prediction_df'],\n",
    "    }, title_suffix=model_name)\n",
    "    # break\n",
    "    without_vaf = model_name == 'predict_mds_without_vaf_select_by_mds_vs_cyto_final_fdr_0.1_no_DEGs'\n",
    "    edf_or_fig_name = 'EDF_11C_' if without_vaf else 'fig_4F_'\n",
    "    out_dir_path = 'temp/_fig_sup_4' if without_vaf else 'temp/_fig4'\n",
    "    pathlib.Path(out_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(os.path.join(out_dir_path, f'{edf_or_fig_name}{model_name}_train_and_test_auc_curves.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "377104351be6dc5fb44deacee428a5d5a5aaea0f9f6aa21ce74220904ae67020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
